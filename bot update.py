import json
import time
import telebot
import requests
import os
from datetime import datetime
from flask import Flask
from threading import Thread
import threading

#Flask –¥–ª—è UptimeRobot
app = Flask(__name__)

@app.route('/')
def home():
    return "Bot is alive!"

def run():
    app.run(host="0.0.0.0", port=3000)

t = Thread(target=run, daemon=True)
t.start()

#–ß—Ç–µ–Ω–∏–µ –∫–ª—é—á–µ–π –∏–∑ secret.txt
def get_keys():
    keys = {"TELEGRAM_TOKEN": None, "GROQ_API_KEY": None}
    
    if os.path.exists("secret.txt"):
        try:
            with open("secret.txt", "r", encoding="utf-8") as f:
                for line in f:
                    line = line.strip()
                    if "=" in line:
                        key, value = line.split("=", 1)
                        key = key.strip()
                        if key in keys:
                            keys[key] = value.strip()
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è secret.txt: {e}")
    
    if not keys["TELEGRAM_TOKEN"]:
        keys["TELEGRAM_TOKEN"] = os.getenv("TELEGRAM_TOKEN")
    if not keys["GROQ_API_KEY"]:
        keys["GROQ_API_KEY"] = os.getenv("GROQ_API_KEY")
    
    return keys["TELEGRAM_TOKEN"], keys["GROQ_API_KEY"]

TOKEN, GROQ_KEY = get_keys()

if not TOKEN:
    print("‚ùå –û–®–ò–ë–ö–ê: –ù–µ—Ç —Ç–æ–∫–µ–Ω–∞ —Ç–µ–ª–µ–≥—Ä–∞–º!")
    exit()

bot = telebot.TeleBot(TOKEN)
@bot.message_handler(commands=['ping'])
def ping(message):
    bot.reply_to(message, "üèì Pong!")
# –ü–∞–º—è—Ç—å –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
MEMORY_FILE = "bot_memory.json"
LOG_FILE = "bot_log.txt"

def log_event(user_id, event_type, details=""):
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    with open(LOG_FILE, "a", encoding="utf-8") as f:
        f.write(f"[{timestamp}] user:{user_id} {event_type} {details}\n")

def load_memory():
    if not os.path.exists(MEMORY_FILE):
        return {}
    backup_file = f"{MEMORY_FILE}.backup"
    try:
        with open(MEMORY_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)
        with open(backup_file, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        return data
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ø–∞–º—è—Ç–∏: {e}")
        if os.path.exists(backup_file):
            try:
                with open(backup_file, "r", encoding="utf-8") as f:
                    data = json.load(f)
                print("‚úÖ –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –∏–∑ —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏")
                return data
            except:
                pass
        return {}

def save_memory(data):
    try:
        with open(MEMORY_FILE, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–∞–º—è—Ç–∏: {e}")

memory = load_memory()

def add_to_memory(user_id, question, answer, model_used=None):
    uid = str(user_id)
    if uid not in memory:
        memory[uid] = []
    memory[uid].append({
        "–≤": question[:2048],
        "–æ": answer[:2048],
        "—Ç": time.time(),
        "–¥": datetime.now().strftime("%Y-%m-%d %H:%M"),
        "–º": model_used or "llama-3.3-70b-versatile"
    })
    if len(memory[uid]) > 30:
        memory[uid] = memory[uid][-30:]
    if len(memory[uid]) % 5 == 0:
        save_memory(memory)
    log_event(user_id, "chat", f"q:{len(question)} a:{len(answer)} model:{model_used}")

def get_user_history(user_id, limit=5):
    uid = str(user_id)
    if uid in memory:
        return memory[uid][-limit:]
    return []

def get_context_from_history(user_id):
    history = get_user_history(user_id, limit=3)
    if not history:
        return ""
    context = "”®—Ç–∫–µ–Ω —Å”©–π–ª–µ—Å—É–ª–µ—Ä—ñ–º—ñ–∑ (–ø—Ä–µ–¥—ã–¥—É—â–∏–µ —Ä–∞–∑–≥–æ–≤–æ—Ä—ã):\n"
    for i, h in enumerate(history, 1):
        context += f"{i}. –ú–µ–Ω: {h['–≤']}\n   –°—ñ–∑: {h['–æ'][:100]}...\n"
    return context

#AI –ú–æ–¥—É–ª—å
class AIModule:
    def __init__(self):
        self.available_models = [
            "llama-3.3-70b-versatile",
            "llama-3.1-8b-instant",
            "mixtral-8x7b-32768",
            "llama-3.2-3b-preview"
        ]
        self.current_model_index = 0
        self.fallback_count = 0

    def get_next_model(self):
        self.current_model_index = (self.current_model_index + 1) % len(self.available_models)
        return self.available_models[self.current_model_index]

    def ask_with_fallback(self, text):
        lang_hint = ""
        if any(char in text for char in "”ô“ì“õ“£”©“±“Ø—ñ”ò“í“ö“¢”®“∞“Æ–Ü"):
            lang_hint = "–°”ô–ª–µ–º–µ—Ç—Å—ñ–∑ –±–µ! –°—ñ–∑ “õ–∞–∑–∞“õ —Ç—ñ–ª—ñ–Ω–¥–µ —Å“±—Ä–∞“õ “õ–æ–π–¥—ã“£—ã–∑. –ñ–∞—É–∞–±—ã“£—ã–∑–¥—ã “õ–∞–∑–∞“õ —Ç—ñ–ª—ñ–Ω–¥–µ –±–µ—Ä—ñ“£—ñ–∑.\n\n"
        elif any(char in text for char in "–∞–±–≤–≥–¥–µ—ë–∂–∑–∏–π–∫–ª–º–Ω–æ–ø—Ä—Å—Ç—É—Ñ—Ö—Ü—á—à—â—ä—ã—å—ç—é—è"):
            lang_hint = "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ! –í–∞—à –≤–æ–ø—Ä–æ—Å –Ω–∞ —Ä—É—Å—Å–∫–æ–º. –û—Ç–≤–µ—á—É –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.\n\n"
        enhanced_prompt = f"{lang_hint}{text}"
        for attempt in range(2):
            current_model = self.available_models[self.current_model_index]
            url = "https://api.groq.com/openai/v1/chat/completions"
            headers = {"Authorization": f"Bearer {GROQ_KEY}", "Content-Type": "application/json"}
            system_message = {"role": "system", "content": "–í—ã –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç, –æ—Ç–≤–µ—á–∞—é—â–∏–π –Ω–∞ –∫–∞–∑–∞—Ö—Å–∫–æ–º –∏ —Ä—É—Å—Å–∫–æ–º."}
            data = {"model": current_model, "messages":[system_message, {"role":"user","content":enhanced_prompt}], "max_tokens":100000, "temperature":0.7, "top_p":0.9}
            try:
                resp = requests.post(url, headers=headers, json=data, timeout=15)
                if resp.status_code == 200:
                    answer = resp.json()["choices"][0]["message"]["content"]
                    return answer, current_model
                else:
                    self.get_next_model()
                    time.sleep(0.3)
            except:
                self.get_next_model()
                time.sleep(0.3)
        # fallback
        fallback_responses = ["–ö–µ—à—ñ—Ä—ñ“£—ñ–∑, “õ–∞–∑—ñ—Ä –∂–∞—É–∞–ø –±–µ—Ä–µ –∞–ª–º–∞–π–º—ã–Ω. –ë—ñ—Ä–∞–∑–¥–∞–Ω —Å–æ“£ “õ–∞–π—Ç–∞–ª–∞–ø –∫”©—Ä—ñ“£—ñ–∑. üòä",
                              "–ò–∑–≤–∏–Ω–∏—Ç–µ, —Å–µ–π—á–∞—Å –Ω–µ –º–æ–≥—É –æ—Ç–≤–µ—Ç–∏—Ç—å. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —á–µ—Ä–µ–∑ –Ω–µ–∫–æ—Ç–æ—Ä–æ–µ –≤—Ä–µ–º—è. üòä",
                              "“ö–∞–∑—ñ—Ä —Å–µ—Ä–≤–µ—Ä–¥–µ “õ–∏—ã–Ω–¥—ã“õ –±–∞—Ä. –ë—ñ—Ä–∞–∑–¥–∞–Ω –∫–µ–π—ñ–Ω —Å“±—Ä–∞“£—ã–∑. üôè"]
        self.fallback_count += 1
        return fallback_responses[self.fallback_count % len(fallback_responses)], "fallback"

ai_module = AIModule()

#–û–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ –±–æ—Ç–∞
@bot.message_handler(commands=['start', 'help'])
def start_msg(msg):
    uid = msg.from_user.id
    history_count = len(get_user_history(uid))
    welcome_text = (
        f"ü§ñ *–°”ô–ª–µ–º! / Hello!*\n\n"
        f"–ú–µ–Ω “õ–∞–∑–∞“õ –∂”ô–Ω–µ –æ—Ä—ã—Å —Ç—ñ–ª–¥–µ—Ä—ñ–Ω–¥–µ —Å”©–π–ª–µ–π—Ç—ñ–Ω –∫”©–º–µ–∫—à—ñ–º—ñ–Ω.\n"
        f"–°—ñ–∑–¥—ñ“£ —Å–æ“£“ì—ã {history_count} —Å“±—Ä–∞“ì—ã“£—ã–∑–¥—ã –µ—Å—ñ–º–¥–µ —Å–∞“õ—Ç–∞–π–º—ã–Ω.\n"
        f"/history - —Å–æ“£“ì—ã —Å“±—Ä–∞“õ—Ç–∞—Ä\n/clear - –∏—Å—Ç–æ—Ä–∏—è —Ç–∞–∑–∞–ª–∞—É\n/stats - —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n/model - –º–æ–¥–µ–ª—å\n/lang - —Ç—ñ–ª\n/help - –ø–æ–º–æ—â—å"
    )
    bot.send_message(msg.chat.id, welcome_text, parse_mode='Markdown')
    log_event(uid, "start")

@bot.message_handler(func=lambda m: True)
def handle_message(msg):
    if msg.text.startswith('/'):
        bot.reply_to(msg, "–ë–µ–ª–≥—ñ—Å—ñ–∑ –∫–æ–º–∞–Ω–¥–∞ / –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –∫–æ–º–∞–Ω–¥–∞")
        return
    uid = msg.from_user.id
    question = msg.text.strip()
    if len(question) < 2:
        bot.reply_to(msg, "”®—Ç–µ “õ—ã—Å“õ–∞ / –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–æ")
        return
    bot.send_chat_action(msg.chat.id, 'typing')
    context = get_context_from_history(uid)
    full_question = f"{context}\n\n–ñ–∞“£–∞ —Å“±—Ä–∞“õ / –ù–æ–≤—ã–π –≤–æ–ø—Ä–æ—Å: {question}"
    answer, model_used = ai_module.ask_with_fallback(full_question)
    add_to_memory(uid, question, answer, model_used)
    bot.reply_to(msg, answer)

#–ê–≤—Ç–æ—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏
def auto_save_memory():
    while True:
        time.sleep(300)
        if memory:
            save_memory(memory)
            print(f"üíæ –ê–≤—Ç–æ—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏: {len(memory)} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π")
save_thread = threading.Thread(target=auto_save_memory, daemon=True)
save_thread.start()

#–ó–∞–ø—É—Å–∫ –±–æ—Ç–∞
print("üöÄ –ë–æ—Ç —É—Å–ø–µ—à–Ω–æ –∑–∞–ø—É—â–µ–Ω!")
bot.infinity_polling()
